{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC2Y77QxCuRO"
      },
      "source": [
        "## Summarizing PDFs  with Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9LLUZ_FEBsF"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain\n",
        "# !pip install pypdf\n",
        "# !pip install openai\n",
        "# !pip install dotenv\n",
        "# !pip install tiktoken\n",
        "# !pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNcaLeQrD9uE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb4a3e3-0f00-43f0-e66b-850a7e57495e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "import sys\n",
        "sys.path.append('../..')\n",
        "\n",
        "!pip install python-dotenv\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrXxfsOGEMst"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "  api_key=os.environ['OPENAI_API_KEY']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YLz84FlShaTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL6cVaqYCrdf"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "import textwrap\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Px6ZVd5CL6x"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature = 0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAoThNE5vmkq"
      },
      "source": [
        "## URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTK6Z_tBvpuv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b0e479bc-bc4f-4c12-b522-7142b9bd807a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The article discusses the concept of building autonomous agents powered by Large Language Models (LLMs). It covers components such as planning, memory, and tool use, along with case studies and proof-of-concept examples. Challenges include the finite context length, reliability of the natural language interface, and long-term planning difficulties. The article also provides references for further reading.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "docs = loader.load()\n",
        "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "chain.run(docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL0QhBMBJPWR"
      },
      "source": [
        "## PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKlyJ1mxIJRW",
        "outputId": "92be5b13-cb03-4c65-f720-f962dc7c4f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  722k  100  722k    0     0  2285k      0 --:--:-- --:--:-- --:--:-- 2287k\n"
          ]
        }
      ],
      "source": [
        "!curl -o paper1.pdf https://arxiv.org/pdf/2402.15061.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4foE0GCDjS_"
      },
      "outputs": [],
      "source": [
        "pdf_path = \"./paper1.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "docs = loader.load_and_split()\n",
        "\n",
        "summarize_chain = load_summarize_chain(llm, chain_type = \"map_reduce\")\n",
        "\n",
        "summary = summarize_chain.run(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX2gfDUYn2bx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d7a390-1fbb-4466-ccbe-fd8a6b64c7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper discusses challenges faced by Large Language Models (LLMs) in domain-specific Machine\n",
            "Translation (MT) and proposes a prompt-oriented fine-tuning method called LlamaIT to address these\n",
            "challenges. The method involves constructing a task-specific mix-domain dataset, fine-tuning the LLM\n",
            "with LoRA, and using zero-shot prompting with instructions to adapt MT tasks to the target domain at\n",
            "inference time. Experimental results show that LlamaIT significantly enhances domain-specific MT\n",
            "capabilities of LLMs while preserving their zero-shot MT capabilities. The study also compares\n",
            "LlamaIT with in-context learning and fine-tuning methods used in current LLM-based MT systems,\n",
            "highlighting the benefits of prompt-oriented fine-tuning for domain-specific translation tasks.\n"
          ]
        }
      ],
      "source": [
        "wrapped_text = textwrap.fill(summary, width= 100,\n",
        "                             break_long_words= False,\n",
        "                             replace_whitespace= False)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du1EUl34F0ie"
      },
      "source": [
        "For summarization tasks, we need three main tools:\n",
        "\n",
        "\n",
        "*   Documents loaders\n",
        "*   Text splitters\n",
        "*   Chain types(Stuff, Map reduce, Refine, Map-Rerank)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1-IYDxIFzOz"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "pdf_path = \"./paper1.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=\"\\n\",\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(docs)\n",
        "\n",
        "summarize_chain = load_summarize_chain(llm, chain_type = \"map_reduce\")\n",
        "\n",
        "summarize_chain.run(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqkO1LHKI0XW"
      },
      "source": [
        "## Personalise your Summarization(by using Prompt Template)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "DLFUSG6Ym-kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbCauNqOIoCP"
      },
      "outputs": [],
      "source": [
        "custom_prompt = \"\"\"Summarize precisely the following  paper into a set of 5 bullets points:\n",
        "                  \\n\\n \" + \" paper : {text}\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template = custom_prompt, input_variables = [\"text\"])\n",
        "\n",
        "summarize_chain = load_summarize_chain(llm, chain_type = \"map_reduce\",\n",
        "                                       map_prompt = prompt, combine_prompt = prompt)\n",
        "summary = summarize_chain.run(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNdrFdj4oVIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97fb9a9c-15ed-45ae-a1e7-8675f2ba62ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Large language models have potential in domain-specific machine translation but face challenges\n",
            "such as sensitivity to input examples and over-generation.\n",
            "- Fine-tuning LLMs on specific domains\n",
            "may improve domain-specific MT performance.\n",
            "- Research is needed to fully utilize LLMs in domain-\n",
            "specific machine translation and address challenges.\n",
            "- ChatGPT 3.5 performs best among models tested\n",
            "in domain-specific MT tasks.\n",
            "- LlamaIT method combines fine-tuning and in-context learning for\n",
            "accurate translation of domain-specific terms.\n"
          ]
        }
      ],
      "source": [
        "wrapped_text = textwrap.fill(summary, width= 100,\n",
        "                             break_long_words= False,\n",
        "                             replace_whitespace= False)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EnApSuEVhCy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2MtUP_0p-2o"
      },
      "source": [
        "## Bonus: Summarizer App via Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "o-hKqEG2p97_",
        "outputId": "6722abaf-6d71-46bf-c687-75f8ffbd3331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://0e073c3d2b80fb768a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0e073c3d2b80fb768a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "\n",
        "# Define the summarization function for URLs\n",
        "def summarize_url(url):\n",
        "    loader = WebBaseLoader(url)\n",
        "    docs = loader.load()\n",
        "    llm = ChatOpenAI(temperature=0)\n",
        "    summarize_chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "    summary = summarize_chain.run(docs)\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "# Gradio interface\n",
        "\n",
        "interface_gradio  = gr.Interface(fn=summarize_url,\n",
        "                     inputs=[gr.Textbox(label=\"URL Text to summarize\", lines=2)],\n",
        "                     outputs=[gr.Textbox(label=\"Summary\", lines=3)],\n",
        "                     title=\"LLM Summarizer\",\n",
        "                     description=\"Paste  your URL to get a summary.\")\n",
        "\n",
        "\n",
        "\n",
        "interface_gradio.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LuBqa-Ote2fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iDlCo49Ge2b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "\n",
        "# # Define the summarization function for PDFs\n",
        "# def summarize_pdf(pdf_file):\n",
        "\n",
        "#     loader = PyPDFLoader(pdf_file)\n",
        "#     docs = loader.load()\n",
        "\n",
        "\n",
        "#     llm = ChatOpenAI(temperature = 0.0)\n",
        "\n",
        "\n",
        "#     text_splitter = RecursiveCharacterTextSplitter(\n",
        "#         separators=\"\\n\",\n",
        "#         chunk_size=500,\n",
        "#         chunk_overlap=150,\n",
        "#         length_function=len\n",
        "#     )\n",
        "#     docs = text_splitter.split_documents(docs)\n",
        "\n",
        "\n",
        "#     summarize_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "#     summary = summarize_chain.run(docs)\n",
        "\n",
        "#     return summary\n",
        "\n",
        "# Define the summarization function for URLs\n",
        "def summarize_url(url):\n",
        "    loader = WebBaseLoader(url)\n",
        "    docs = loader.load()\n",
        "\n",
        "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-1106\")\n",
        "\n",
        "    summarize_chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "    summary = summarize_chain.run(docs)\n",
        "\n",
        "    return summary\n",
        "\n",
        "# # Define the main function that the Gradio interface will use\n",
        "# def summarize(input_type, input_data):\n",
        "#     if input_type == \"PDF\":\n",
        "#         return summarize_pdf(input_data)\n",
        "#     elif input_type == \"URL\":\n",
        "#         return summarize_url(input_data)\n",
        "#     else:\n",
        "#         return \"Unsupported input type.\"\n",
        "\n",
        "# Gradio interface\n",
        "# demo = gr.Interface(\n",
        "#     fn=summarize,\n",
        "#     inputs=[\n",
        "#         gr.Radio([\"PDF\", \"URL\"], label=\"Select Input Type\", type=\"index\"),\n",
        "#         gr.Data(file=True, type=\"file\", label=\"Upload PDF or Enter URL\")\n",
        "#         gr.Textbox(lines=5, placeholder=\"Enter PDF path, URL, or Notion link here...\")\n",
        "#     ],\n",
        "#     outputs='text',\n",
        "#     title=\"LLM Summarizer\",\n",
        "#     description=\"Upload a PDF or provide a URL to get a summary.\"\n",
        "# )\n",
        "\n",
        "\n",
        "interface_gradio  = gr.Interface(fn=summarize_url,\n",
        "                     inputs=[gr.Textbox(label=\"Text to summarize\", lines=6)],\n",
        "                     outputs=[gr.Textbox(label=\"Summary\", lines=3)],\n",
        "                     title=\"LLM Summarizer\",\n",
        "                     description=\"Paste  your URL to get a summary.\")\n",
        "\n",
        "\n",
        "\n",
        "interface_gradio.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "id": "J4VCfknUe2ZI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}